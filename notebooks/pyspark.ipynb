{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fdfae8-f65a-4f31-9fac-055df0778c92",
   "metadata": {},
   "source": [
    "# Integración con **PySpark**\n",
    "Otra alternativa para poder integrar todo este proyecto en conjunto y dentro todo mismo usando python puede ser con [PySpark](https://spark.apache.org/docs/latest/api/python/index.html). A continuación se detalla lo necesario para poder trabajar con esta librería y se analiza esta opción.\n",
    "\n",
    "## Uso\n",
    "Para iniciar a usar pyspark, es necesario primero crear una sesión de la siguiente forma (es necesario tener **Java** instalado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9e0c86-ff1f-4cba-841e-c38c40a30d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/03 16:36:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bd2c9-9756-49c7-a6c8-2c33ef25a77f",
   "metadata": {},
   "source": [
    "*Algunos *warnings* fueron arrojados, pero no parecen ser vitales.*\n",
    "\n",
    "Pyspark trabaja de una manera más eficiente con `DataFrames` (usados también en Pandas) y estos pueden ser obtenidos a través de archivos que contengan fuentes de datos. En este caso, utilizarémos un ejemplo básico de un `geojson` (`JSON`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5d0f4-56e9-4eb2-a774-fa1c37de0c87",
   "metadata": {},
   "source": [
    "### Leyendo `JSON` (`geojson`)\n",
    "Una vez teniendo una sesión de PySpark es posible leer `JSON`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7ee657-54ec-4f1a-8149-f7b3a334c0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|         coordinates|           type|\n",
      "+--------------------+---------------+\n",
      "|[[[1.0, 2.0]], [[...|MultiLineString|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geojson_df = spark.read.json(\"../assets/basic.json\", multiLine=True)\n",
    "geojson_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de7f3a-398c-4cce-b44c-318878112b71",
   "metadata": {},
   "source": [
    "Casi de la misma forma que se leen los `JSON`s en `Python`. Es importante también considerar la bandera `multiLine`, ya que si no se establece esta, habrán errores de lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d7ab42-4c40-472d-9066-07ee06802ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'MultiLineString', 'coordinates': [[[1.0, 2.0]], [[3.0, 4.0]]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leyendo json en Python\n",
    "import json\n",
    "\n",
    "with open('../assets/basic.json', 'r') as file:\n",
    "    example_geojson = json.load(file)\n",
    "\n",
    "example_geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d13809-57e1-47ff-895f-0aa396875666",
   "metadata": {},
   "source": [
    "### Cálculo de distancia euclideana\n",
    "Teniendo el `JSON` (o `geojson`) cargado, se pueden definir funciones con las que PySpark podrá trabajar, denomidas [UDF (User Defined Functions)](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html), para así poder definir una función que calcule la distancia euclideana de las coordenadas que nuestro `JSON` contenga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db611e5-0741-493c-950d-0cd6888ea1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------+--------+\n",
      "|coordinates                 |type           |distance|\n",
      "+----------------------------+---------------+--------+\n",
      "|[[[1.0, 2.0]], [[3.0, 4.0]]]|MultiLineString|2.828427|\n",
      "+----------------------------+---------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "from math import sqrt\n",
    "\n",
    "# Define UDF to calculate Euclidean distance\n",
    "def euclidean_distance(coords):\n",
    "    point1 = coords[0][0]\n",
    "    point2 = coords[1][0]\n",
    "    return float(sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2))\n",
    "\n",
    "distance_udf = F.udf(euclidean_distance, FloatType())\n",
    "\n",
    "geojson_df = geojson_df.withColumn('distance', distance_udf(F.col('coordinates')))\n",
    "geojson_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
