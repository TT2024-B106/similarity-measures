{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fdfae8-f65a-4f31-9fac-055df0778c92",
   "metadata": {},
   "source": [
    "# Integración con **PySpark**\n",
    "Otra alternativa para poder integrar todo este proyecto en conjunto y dentro todo mismo usando python puede ser con [PySpark](https://spark.apache.org/docs/latest/api/python/index.html). A continuación se detalla lo necesario para poder trabajar con esta librería y se analiza esta opción.\n",
    "\n",
    "## Uso\n",
    "Para iniciar a usar pyspark, es necesario primero crear una sesión de la siguiente forma (es necesario tener **Java** instalado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9e0c86-ff1f-4cba-841e-c38c40a30d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|         coordinates|           type|\n",
      "+--------------------+---------------+\n",
      "|[[[1.0, 2.0]], [[...|MultiLineString|\n",
      "+--------------------+---------------+\n",
      "\n",
      "+--------------------+\n",
      "|              coords|\n",
      "+--------------------+\n",
      "|[[[1.0, 2.0]], [[...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Fast testing\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "geojson_df = spark.read.json(\"../assets/basic.json\", multiLine=True)\n",
    "coordinates_df = geojson_df.select(col(\"coordinates\").alias(\"coords\"))\n",
    "\n",
    "geojson_df.show()\n",
    "coordinates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db611e5-0741-493c-950d-0cd6888ea1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------+--------+\n",
      "|coordinates                 |type           |distance|\n",
      "+----------------------------+---------------+--------+\n",
      "|[[[1.0, 2.0]], [[3.0, 4.0]]]|MultiLineString|2.828427|\n",
      "+----------------------------+---------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType, ArrayType, StructType, StructField, StringType\n",
    "from math import sqrt\n",
    "\n",
    "# Define UDF to calculate Euclidean distance\n",
    "def euclidean_distance(coords):\n",
    "    point1 = coords[0][0]\n",
    "    point2 = coords[1][0]\n",
    "    return float(sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2))\n",
    "\n",
    "distance_udf = F.udf(euclidean_distance, FloatType())\n",
    "\n",
    "geojson_df = geojson_df.withColumn('distance', distance_udf(F.col('coordinates')))\n",
    "geojson_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28de36-af6b-4288-a3cd-f4c39b71c179",
   "metadata": {},
   "source": [
    "Algunos *warnings* fueron arrojados, pero no parecen ser vitales.\n",
    "\n",
    "Pyspark trabaja de una manera más eficiente con `DataFrames` (usados también en Pandas) y estos pueden ser obtenidos a través de archivos que contengan fuentes de datos. En este caso, utilizarémos un ejemplo básico de un geojson (json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51373685-4cf1-48ca-8ffe-740cc8c22fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'MultiLineString', 'coordinates': [[[1.0, 2.0]], [[3.0, 4.0]]]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leyendo json en Python\n",
    "import json\n",
    "\n",
    "with open('../assets/basic.json', 'r') as file:\n",
    "    example_geojson = json.load(file)\n",
    "\n",
    "example_geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d76c8e8-0e7f-468b-9651-cb6b6537e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.0, 2.0]], [[3.0, 4.0]]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "geojson_df = spark.read.json(\"../assets/basic.json\", multiLine=True)\n",
    "coordinates_df = geojson_df.select(col(\"coordinates\").alias(\"coords\"))\n",
    "print(coordinates_df.rdd.flatMap(lambda x: x.coords).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e57f2038-797d-4225-a852-22ce198c4daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|feature|\n",
      "+-------+\n",
      "+-------+\n",
      "\n",
      "+-----------+\n",
      "|coordinates|\n",
      "+-----------+\n",
      "+-----------+\n",
      "\n",
      "+----+----+--------+---------------+\n",
      "|id  |type|features|_corrupt_record|\n",
      "+----+----+--------+---------------+\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "|NULL|NULL|NULL    |NULL           |\n",
      "+----+----+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- id: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- properties: struct (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- tiempo: string (nullable = true)\n",
      " |    |    |    |-- geometry: struct (nullable = true)\n",
      " |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |    |-- element: float (containsNull = true)\n",
      " |-- _corrupt_record: void (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leyendo JSON con PySpark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType, FloatType\n",
    "from pyspark.sql.functions import explode, col, lit\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"features\", ArrayType(StructType([\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"properties\", StructType([\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"tiempo\", StringType(), True),\n",
    "            StructField(\"geometry\", StructType([\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"coordinates\", ArrayType(ArrayType(FloatType())), True)\n",
    "            ]))\n",
    "        ]))\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "# Read the GeoJSON file with the defined schema\n",
    "example_geojson = spark.read.schema(schema).json(\"../assets/pyspark_example.geojson\")\n",
    "# Handle corrupt records\n",
    "example_geojson = example_geojson.withColumn(\"_corrupt_record\", lit(None))\n",
    "\n",
    "# Check for corrupt records\n",
    "corrupt_records = example_geojson.filter(example_geojson._corrupt_record.isNotNull())\n",
    "if corrupt_records.count() > 0:\n",
    "    corrupt_records.show(truncate=False)\n",
    "else:\n",
    "    # Explode the 'features' array to get each feature as a separate row\n",
    "    features = example_geojson.select(explode(\"features\").alias(\"feature\"))\n",
    "    features.show()\n",
    "    \n",
    "    # Select the coordinates from each feature\n",
    "    coordinates = features.select(col(\"feature.properties.geometry.coordinates\").alias(\"coordinates\"))\n",
    "\n",
    "    # Show the coordinates\n",
    "    coordinates.show(truncate=False)\n",
    "\n",
    "example_geojson.show(truncate=False)\n",
    "example_geojson.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "232fc3e7-be29-4203-9565-27639a2cd30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+\n",
      "|  id|type|features|\n",
      "+----+----+--------+\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "|NULL|NULL|    NULL|\n",
      "+----+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StructType([\n",
    "        StructField(\"$oid\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"features\", ArrayType(StructType([\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"properties\", StructType([\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"tiempo\", StringType(), True),\n",
    "            StructField(\"geometry\", StructType([\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"coordinates\", ArrayType(ArrayType(FloatType())), True)\n",
    "            ]))\n",
    "        ]))\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "# Read the GeoJSON file with the defined schema\n",
    "example_geojson = spark.read.schema(schema) \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "    .json(\"../assets/pyspark_example.geojson\")\n",
    "\n",
    "example_geojson.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4fc687-37d7-4c87-8117-35f1c1bc2024",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `features` cannot be resolved. Did you mean one of the following? [`_corrupt_record`].;\n'Project [explode('features) AS feature#10]\n+- Relation [_corrupt_record#8] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explode, col\n\u001b[0;32m----> 3\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mexample_geojson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexplode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:3227\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3227\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `features` cannot be resolved. Did you mean one of the following? [`_corrupt_record`].;\n'Project [explode('features) AS feature#10]\n+- Relation [_corrupt_record#8] json\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "features = example_geojson.select(explode(\"features\").alias(\"feature\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0f971-6d3c-4776-89b5-efa72c3e686d",
   "metadata": {},
   "source": [
    "Ahora con esto, se pueden acceder a las coordenadas de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f52d96ca-b03c-415c-8f51-5f71eae44f4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'explode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, explode\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Explode the 'features' array to access individual features\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m exploded_df \u001b[38;5;241m=\u001b[39m \u001b[43mexample_geojson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplode\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Extract coordinates as an array of arrays\u001b[39;00m\n\u001b[1;32m      7\u001b[0m coordinates_df \u001b[38;5;241m=\u001b[39m exploded_df\u001b[38;5;241m.\u001b[39mselect(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures.geometry.coordinates\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:3127\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \n\u001b[1;32m   3096\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3124\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3129\u001b[0m     )\n\u001b[1;32m   3130\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'explode'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# Explode the 'features' array to access individual features\n",
    "exploded_df = example_geojson.explode(\"features\")\n",
    "\n",
    "# Extract coordinates as an array of arrays\n",
    "coordinates_df = exploded_df.select(col(\"features.geometry.coordinates\").alias(\"coordinates\"))\n",
    "\n",
    "# Show the DataFrame with coordinates\n",
    "coordinates_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb772193-3391-4cbb-9966-a89a898bb31e",
   "metadata": {},
   "source": [
    "Algunos *warnings* fueron arrojados, pero no parecen ser vitales.\n",
    "\n",
    "Una vez teniendo la sesión inicializada, se crea la función usando **UDF**: **U**ser-**D**efined **F**unction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a81529-e029-4031-95eb-cdbfbb19d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a User-Defined Function (UDF) for Euclidean distance\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "  # Assuming point1 and point2 are lists with the same length\n",
    "  if len(point1) != len(point2):\n",
    "    raise ValueError(\"Points must have the same number of dimensions\")\n",
    "  \n",
    "  # Calculate the squared difference of each dimension\n",
    "  squared_differences = [(x - y) ** 2 for x, y in zip(point1, point2)]\n",
    "  # Sum the squared differences\n",
    "  distance = sum(squared_differences)\n",
    "  # Take the square root of the sum (Euclidean distance)\n",
    "  return distance ** 0.5  # More efficient than sqrt()\n",
    "\n",
    "# Register the UDF with a specific return type (DoubleType)\n",
    "euclidean_distance_udf = udf(euclidean_distance, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475ee0a-bc0a-4b6d-9f55-fb91b79b21ad",
   "metadata": {},
   "source": [
    "Y ahora esto se puede calcular de la siguiente forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd86239a-1e03-4554-a45d-b7962b1c8f9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m point2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Import the UDF (assuming it's defined in the same Python file)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from my_functions import euclidean_distance_udf  # Replace with your file path\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Call the UDF with the sample points\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m distance \u001b[38;5;241m=\u001b[39m \u001b[43meuclidean_distance_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Print the calculated distance\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuclidean Distance:\u001b[39m\u001b[38;5;124m\"\u001b[39m, distance)\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/sql/udf.py:423\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, assigned\u001b[38;5;241m=\u001b[39massignments)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/sql/udf.py:401\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf\n\u001b[0;32m--> 401\u001b[0m     jPythonUDF \u001b[38;5;241m=\u001b[39m judf\u001b[38;5;241m.\u001b[39mapply(\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jPythonUDF)\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/sql/column.py:88\u001b[0m, in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[0;32m---> 88\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/sql/column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m     66\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list."
     ]
    }
   ],
   "source": [
    "# Sample points as Python lists\n",
    "point1 = [1, 2, 3]\n",
    "point2 = [4, 5, 6]\n",
    "\n",
    "# Import the UDF (assuming it's defined in the same Python file)\n",
    "# from my_functions import euclidean_distance_udf  # Replace with your file path\n",
    "\n",
    "# Call the UDF with the sample points\n",
    "distance = euclidean_distance_udf(point1, point2)\n",
    "\n",
    "# Print the calculated distance\n",
    "print(\"Euclidean Distance:\", distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
