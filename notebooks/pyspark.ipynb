{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fdfae8-f65a-4f31-9fac-055df0778c92",
   "metadata": {},
   "source": [
    "# Integración con **PySpark**\n",
    "Otra alternativa para poder integrar todo este proyecto en conjunto y dentro todo mismo usando python puede ser con [PySpark](https://spark.apache.org/docs/latest/api/python/index.html). A continuación se detalla lo necesario para poder trabajar con esta librería y se analiza esta opción.\n",
    "\n",
    "## Uso\n",
    "Para iniciar a usar pyspark, es necesario primero crear una sesión de la siguiente forma (es necesario tener **Java** instalado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9e0c86-ff1f-4cba-841e-c38c40a30d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/21 15:31:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bd2c9-9756-49c7-a6c8-2c33ef25a77f",
   "metadata": {},
   "source": [
    "*Algunos *warnings* fueron arrojados, pero no parecen ser vitales.*\n",
    "\n",
    "Pyspark trabaja de una manera más eficiente con `DataFrames` (usados también en Pandas) y estos pueden ser obtenidos a través de archivos que contengan fuentes de datos. En este caso, utilizarémos un ejemplo básico de un `geojson` (`JSON`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5d0f4-56e9-4eb2-a774-fa1c37de0c87",
   "metadata": {},
   "source": [
    "### Leyendo `JSON` (`geojson`)\n",
    "Una vez teniendo una sesión de PySpark es posible leer `JSON`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7ee657-54ec-4f1a-8149-f7b3a334c0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|         coordinates|           type|\n",
      "+--------------------+---------------+\n",
      "|[[[1.0, 2.0]], [[...|MultiLineString|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('coordinates', ArrayType(ArrayType(ArrayType(DoubleType(), True), True), True), True), StructField('type', StringType(), True)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geojson_df = spark.read.json(\"../assets/basic.json\", multiLine=True)\n",
    "geojson_df.show()\n",
    "geojson_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de7f3a-398c-4cce-b44c-318878112b71",
   "metadata": {},
   "source": [
    "Casi de la misma forma que se leen los `JSON`s en `Python`. Es importante también considerar la bandera `multiLine`, ya que si no se establece esta, habrán errores de lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d7ab42-4c40-472d-9066-07ee06802ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'MultiLineString', 'coordinates': [[[1.0, 2.0]], [[3.0, 4.0]]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leyendo json en Python\n",
    "import json\n",
    "\n",
    "with open('../assets/basic.json', 'r') as file:\n",
    "    example_geojson = json.load(file)\n",
    "\n",
    "example_geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d13809-57e1-47ff-895f-0aa396875666",
   "metadata": {},
   "source": [
    "### Cálculo de distancia euclideana\n",
    "Teniendo el `JSON` (o `geojson`) cargado, se pueden definir funciones con las que PySpark podrá trabajar, denomidas [UDF (User Defined Functions)](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html), para así poder definir una función que calcule la distancia euclideana de las coordenadas que nuestro `JSON` contenga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db611e5-0741-493c-950d-0cd6888ea1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------+--------+\n",
      "|coordinates                 |type           |distance|\n",
      "+----------------------------+---------------+--------+\n",
      "|[[[1.0, 2.0]], [[3.0, 4.0]]]|MultiLineString|2.828427|\n",
      "+----------------------------+---------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "from math import sqrt\n",
    "\n",
    "# Define UDF to calculate Euclidean distance\n",
    "def euclidean_distance(coords):\n",
    "    point1 = coords[0][0]\n",
    "    point2 = coords[1][0]\n",
    "    return float(sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2))\n",
    "\n",
    "distance_udf = F.udf(euclidean_distance, FloatType())\n",
    "\n",
    "geojson_df = geojson_df.withColumn('distance', distance_udf(F.col('coordinates')))\n",
    "geojson_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3f7d5-efc7-427f-8370-ed52bb46b4bf",
   "metadata": {},
   "source": [
    "## Análisis\n",
    "Ahora se verán los tiempos de ejecución que esta función tiene, para observar si realmente conviene el uso de Pyspark.\n",
    "\n",
    "Primero se importan los `utils` que se tienen dentro del repositorio, para poder generar datos y graficar las respectivas gráficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1d8edd-25d8-48fa-ae2c-ac7d0dd6ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "\n",
    "parent_dir = os.path.join(os.path.abspath(''), os.path.pardir)\n",
    "sys.path.append(os.path.abspath(parent_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962883d-6630-4ebc-a663-311e9453d4a6",
   "metadata": {},
   "source": [
    "De esta forma es posible importar, por ejemplo, la librería interna que se desarrollo para poder generar datos GeoJSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d894021-0750-4530-944c-da99b51722c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------+---------------+\n",
      "|coordinates                                                                           |type           |\n",
      "+--------------------------------------------------------------------------------------+---------------+\n",
      "|[[[-99.47766508692867, 19.304774982417186], [-99.25753858187457, 19.373344147661133]]]|MultiLineString|\n",
      "+--------------------------------------------------------------------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import genjson\n",
    "\n",
    "genjson.dataframe(1, spark).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbee8ac-4b48-4edc-89f3-817bb82f6a0b",
   "metadata": {},
   "source": [
    "### Gráficas de la distancia euclideana con PySpark\n",
    "Ahora se adapta el algoritmo de distancia euclideana para poder utilizarlo con `n` puntos que representarán una trayectoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a41c23ef-f5b4-47a4-bed2-cc801371ec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+------------------+\n",
      "|         coordinates|           type|          distance|\n",
      "+--------------------+---------------+------------------+\n",
      "|[[[-99.1549379792...|MultiLineString|0.5905478928269462|\n",
      "+--------------------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def euclidean_distance(coords: list[list[list[float]]]):\n",
    "    total_distance = 0.0\n",
    "\n",
    "    p = coords[0][0]\n",
    "    q = coords[0][1]\n",
    "\n",
    "    for i in range(len(p)):\n",
    "        total_distance += (q[i] - p[i])**2\n",
    "    \n",
    "    return float(sqrt(total_distance))\n",
    "\n",
    "geojson_df = genjson.dataframe(4, spark)\n",
    "\n",
    "distance_udf = F.udf(euclidean_distance, DoubleType())\n",
    "# This does the calculation + adding a column in dataframe\n",
    "geojson_df = geojson_df.withColumn('distance', distance_udf(F.col('coordinates')))\n",
    "# Here calculation ends, proceeding to show dataframe with new column\n",
    "geojson_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b979f9e-f380-4c22-9d76-44e30dfe0d99",
   "metadata": {},
   "source": [
    "Y con esto es posible graficar con las funciones antes utilizadas en algunos otros análisis hechos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d714eb2-2bd3-489d-a470-af9985b8bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/21 15:31:51 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from utils import plotfn\n",
    "\n",
    "plotfn.time_complexity3_spark(\n",
    "    distance_udf,\n",
    "    genjson.dataframe,\n",
    "    spark,\n",
    "    \"PySpark\",\n",
    "    3000,\n",
    "    3000,\n",
    "    True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
