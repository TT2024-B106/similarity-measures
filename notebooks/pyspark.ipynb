{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fdfae8-f65a-4f31-9fac-055df0778c92",
   "metadata": {},
   "source": [
    "# Integración con **PySpark**\n",
    "Otra alternativa para poder integrar todo este proyecto en conjunto y dentro todo mismo usando python puede ser con [PySpark](https://spark.apache.org/docs/latest/api/python/index.html). A continuación se detalla lo necesario para poder trabajar con esta librería y se analiza esta opción.\n",
    "\n",
    "## Uso\n",
    "Para iniciar a usar pyspark, es necesario primero crear una sesión de la siguiente forma (es necesario tener **Java** instalado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9e0c86-ff1f-4cba-841e-c38c40a30d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/25 20:44:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/25 20:44:56 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28de36-af6b-4288-a3cd-f4c39b71c179",
   "metadata": {},
   "source": [
    "Algunos *warnings* fueron arrojados, pero no parecen ser vitales.\n",
    "\n",
    "Pyspark trabaja de una manera más eficiente con `DataFrames` (usados también en Pandas) y estos pueden ser obtenidos a través de archivos que contengan fuentes de datos. En este caso, utilizarémos un ejemplo básico de un geojson (json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51373685-4cf1-48ca-8ffe-740cc8c22fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': {'$oid': 'exid1'},\n",
       " 'type': 'FeatureCollection',\n",
       " 'features': [{'type': 'Feature',\n",
       "   'properties': {'name': 'exftnm1',\n",
       "    'tiempo': 'exfttm1',\n",
       "    'geometry': {'type': 'LineString', 'coordinates': [[1, 2]]}}},\n",
       "  {'type': 'Feature',\n",
       "   'properties': {'name': 'exftnm2',\n",
       "    'tiempo': 'exfttm2',\n",
       "    'geometry': {'type': 'LineString', 'coordinates': [[3, 4]]}}}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leyendo json en Python\n",
    "import json\n",
    "\n",
    "with open('../assets/pyspark_example.geojson', 'r') as file:\n",
    "    example_geojson = json.load(file)\n",
    "\n",
    "example_geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e57f2038-797d-4225-a852-22ce198c4daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leyendo JSON con PySpark\n",
    "example_geojson = spark.read.json(\"../assets/pyspark_example.geojson\")\n",
    "example_geojson.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb772193-3391-4cbb-9966-a89a898bb31e",
   "metadata": {},
   "source": [
    "Algunos *warnings* fueron arrojados, pero no parecen ser vitales.\n",
    "\n",
    "Una vez teniendo la sesión inicializada, se crea la función usando **UDF**: **U**ser-**D**efined **F**unction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a81529-e029-4031-95eb-cdbfbb19d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a User-Defined Function (UDF) for Euclidean distance\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "  # Assuming point1 and point2 are lists with the same length\n",
    "  if len(point1) != len(point2):\n",
    "    raise ValueError(\"Points must have the same number of dimensions\")\n",
    "  \n",
    "  # Calculate the squared difference of each dimension\n",
    "  squared_differences = [(x - y) ** 2 for x, y in zip(point1, point2)]\n",
    "  # Sum the squared differences\n",
    "  distance = sum(squared_differences)\n",
    "  # Take the square root of the sum (Euclidean distance)\n",
    "  return distance ** 0.5  # More efficient than sqrt()\n",
    "\n",
    "# Register the UDF with a specific return type (DoubleType)\n",
    "euclidean_distance_udf = udf(euclidean_distance, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475ee0a-bc0a-4b6d-9f55-fb91b79b21ad",
   "metadata": {},
   "source": [
    "Y ahora esto se puede calcular de la siguiente forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd86239a-1e03-4554-a45d-b7962b1c8f9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m point2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Import the UDF (assuming it's defined in the same Python file)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from my_functions import euclidean_distance_udf  # Replace with your file path\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Call the UDF with the sample points\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m distance \u001b[38;5;241m=\u001b[39m \u001b[43meuclidean_distance_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Print the calculated distance\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuclidean Distance:\u001b[39m\u001b[38;5;124m\"\u001b[39m, distance)\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/sql/udf.py:423\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, assigned\u001b[38;5;241m=\u001b[39massignments)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/sql/udf.py:401\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf\n\u001b[0;32m--> 401\u001b[0m     jPythonUDF \u001b[38;5;241m=\u001b[39m judf\u001b[38;5;241m.\u001b[39mapply(\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jPythonUDF)\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/sql/column.py:88\u001b[0m, in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[0;32m---> 88\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m~/devel/similarity-measures/venv/lib/python3.12/site-packages/pyspark/sql/column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m     66\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list."
     ]
    }
   ],
   "source": [
    "# Sample points as Python lists\n",
    "point1 = [1, 2, 3]\n",
    "point2 = [4, 5, 6]\n",
    "\n",
    "# Import the UDF (assuming it's defined in the same Python file)\n",
    "# from my_functions import euclidean_distance_udf  # Replace with your file path\n",
    "\n",
    "# Call the UDF with the sample points\n",
    "distance = euclidean_distance_udf(point1, point2)\n",
    "\n",
    "# Print the calculated distance\n",
    "print(\"Euclidean Distance:\", distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
